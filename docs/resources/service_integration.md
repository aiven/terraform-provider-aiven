---
page_title: "aiven_service_integration Resource - terraform-provider-aiven"
subcategory: ""
description: |-
  Creates and manages an Aiven service integration https://aiven.io/docs/platform/concepts/service-integration.
---
# aiven_service_integration (Resource)
Creates and manages an Aiven [service integration](https://aiven.io/docs/platform/concepts/service-integration).

-> Services integrations are not supported for services running on hobbyist plans.

You can set up an integration between two Aiven services or an Aiven service and an external
service. For example, you can send metrics from a Kafka service to an M3DB service,
send metrics from an M3DB service to a Grafana service to show dashboards, and send logs from
any service to OpenSearch. For external integrations, you also need an
[integration endpoint](https://registry.terraform.io/providers/aiven/aiven/latest/docs/resources/service_integration_endpoint).

You can also use service integrations to enable and use the [disk autoscaler](https://registry.terraform.io/providers/aiven/aiven/latest/docs/guides/disk-autoscaler).

~> **Warning**
For services managed by Terraform, removing an autoscaler integration on services with `additional_disk_space` resets the service disk space to the service plan's disk size.
See [Remove the autoscaler](/providers/aiven/aiven/latest/docs/guides/disk-autoscaler#remove-the-autoscaler) for more.

## Example Usage
```terraform
# Integrate Kafka and Thanos services for metrics
resource "aiven_service_integration" "example_integration" {
  project                  = data.aiven_project.example_project.project
  integration_type         = "metrics"
  source_service_name      = aiven_kafka.example_kafka.service_name
  destination_service_name = aiven_thanos.example_thanos.service_name
}

# Use disk autoscaler with a PostgreSQL service
resource "aiven_service_integration_endpoint" "autoscaler_endpoint" {
  project       = data.aiven_project.example_project.project
  endpoint_name = "disk-autoscaler-200GiB"
  endpoint_type = "autoscaler"

  autoscaler_user_config {
    autoscaling {
      cap_gb = 200
      type   = "autoscale_disk"
    }
  }
}

resource "aiven_service_integration" "autoscaler_integration" {
  project                 = data.aiven_project.example_project.project
  integration_type        = "autoscaler"
  source_service_name     = aiven_pg.example_pg.service_name
  destination_endpoint_id = aiven_service_integration_endpoint.autoscaler_endpoint.id
}
```
<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `integration_type` (String) Type of the service integration. The possible values are `alertmanager`, `autoscaler`, `caching`, `cassandra_cross_service_cluster`, `clickhouse_credentials`, `clickhouse_kafka`, `clickhouse_postgresql`, `dashboard`, `datadog`, `datasource`, `disaster_recovery`, `external_aws_cloudwatch_logs`, `external_aws_cloudwatch_metrics`, `external_elasticsearch_logs`, `external_google_cloud_logging`, `external_opensearch_logs`, `flink`, `flink_external_bigquery`, `flink_external_kafka`, `flink_external_postgresql`, `internal_connectivity`, `jolokia`, `kafka_connect`, `kafka_connect_postgresql`, `kafka_logs`, `kafka_mirrormaker`, `logs`, `m3aggregator`, `m3coordinator`, `metrics`, `opensearch_cross_cluster_replication`, `opensearch_cross_cluster_search`, `prometheus`, `read_replica`, `rsyslog`, `schema_registry_proxy`, `stresstester`, `thanos_distributed_query`, `thanos_migrate`, `thanoscompactor`, `thanosquery`, `thanosruler`, `thanosstore`, `vector` and `vmalert`.
- `project` (String) Project the integration belongs to.

### Optional

- `clickhouse_credentials_user_config` (Block List, Max: 1) ClickhouseCredentials user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--clickhouse_credentials_user_config))
- `clickhouse_kafka_user_config` (Block List, Max: 1) ClickhouseKafka user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--clickhouse_kafka_user_config))
- `clickhouse_postgresql_user_config` (Block List, Max: 1) ClickhousePostgresql user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--clickhouse_postgresql_user_config))
- `datadog_user_config` (Block List, Max: 1) Datadog user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--datadog_user_config))
- `destination_endpoint_id` (String) Destination endpoint for the integration.
- `destination_service_name` (String) Destination service for the integration.
- `destination_service_project` (String) Destination service project name
- `external_aws_cloudwatch_logs_user_config` (Block List, Max: 1) ExternalAwsCloudwatchLogs user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--external_aws_cloudwatch_logs_user_config))
- `external_aws_cloudwatch_metrics_user_config` (Block List, Max: 1) ExternalAwsCloudwatchMetrics user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--external_aws_cloudwatch_metrics_user_config))
- `external_elasticsearch_logs_user_config` (Block List, Max: 1) ExternalElasticsearchLogs user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--external_elasticsearch_logs_user_config))
- `external_opensearch_logs_user_config` (Block List, Max: 1) ExternalOpensearchLogs user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--external_opensearch_logs_user_config))
- `flink_external_postgresql_user_config` (Block List, Max: 1) FlinkExternalPostgresql user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--flink_external_postgresql_user_config))
- `kafka_connect_user_config` (Block List, Max: 1) KafkaConnect user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--kafka_connect_user_config))
- `kafka_logs_user_config` (Block List, Max: 1) KafkaLogs user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--kafka_logs_user_config))
- `kafka_mirrormaker_user_config` (Block List, Max: 1) KafkaMirrormaker user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--kafka_mirrormaker_user_config))
- `logs_user_config` (Block List, Max: 1) Logs user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--logs_user_config))
- `metrics_user_config` (Block List, Max: 1) Metrics user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--metrics_user_config))
- `prometheus_user_config` (Block List, Max: 1) Prometheus user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later (see [below for nested schema](#nestedblock--prometheus_user_config))
- `source_endpoint_id` (String) Source endpoint for the integration.
- `source_service_name` (String) Source service for the integration (if any)
- `source_service_project` (String) Source service project name
- `timeouts` (Block, Optional) (see [below for nested schema](#nestedblock--timeouts))

### Read-Only

- `id` (String) The ID of this resource.
- `integration_id` (String) The ID of the Aiven service integration.

<a id="nestedblock--clickhouse_credentials_user_config"></a>
### Nested Schema for `clickhouse_credentials_user_config`

Optional:

- `grants` (Block List, Max: 64) Grants to assign (see [below for nested schema](#nestedblock--clickhouse_credentials_user_config--grants))

<a id="nestedblock--clickhouse_credentials_user_config--grants"></a>
### Nested Schema for `clickhouse_credentials_user_config.grants`

Required:

- `user` (String) User or role to assign the grant to. Example: `alice`.



<a id="nestedblock--clickhouse_kafka_user_config"></a>
### Nested Schema for `clickhouse_kafka_user_config`

Optional:

- `tables` (Block List, Max: 400) Tables to create (see [below for nested schema](#nestedblock--clickhouse_kafka_user_config--tables))

<a id="nestedblock--clickhouse_kafka_user_config--tables"></a>
### Nested Schema for `clickhouse_kafka_user_config.tables`

Required:

- `columns` (Block List, Min: 1, Max: 100) Table columns (see [below for nested schema](#nestedblock--clickhouse_kafka_user_config--tables--columns))
- `data_format` (String) Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. Message data format. Default: `JSONEachRow`.
- `group_name` (String) Kafka consumers group. Default: `clickhouse`.
- `name` (String) Name of the table. Example: `events`.
- `topics` (Block List, Min: 1, Max: 100) Kafka topics (see [below for nested schema](#nestedblock--clickhouse_kafka_user_config--tables--topics))

Optional:

- `auto_offset_reset` (String) Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Action to take when there is no initial offset in offset store or the desired offset is out of range. Default: `earliest`.
- `date_time_input_format` (String) Enum: `basic`, `best_effort`, `best_effort_us`. Method to read DateTime from text input formats. Default: `basic`.
- `handle_error_mode` (String) Enum: `default`, `stream`. How to handle errors for Kafka engine. Default: `default`.
- `max_block_size` (Number) Number of row collected by poll(s) for flushing data from Kafka. Default: `0`.
- `max_rows_per_message` (Number) The maximum number of rows produced in one kafka message for row-based formats. Default: `1`.
- `num_consumers` (Number) The number of consumers per table per replica. Default: `1`.
- `poll_max_batch_size` (Number) Maximum amount of messages to be polled in a single Kafka poll. Default: `0`.
- `poll_max_timeout_ms` (Number) Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream_flush_interval_ms server setting by default (500ms). Default: `0`.
- `producer_batch_num_messages` (Number) The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.
- `producer_batch_size` (Number) The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.
- `producer_compression_codec` (String) Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.
- `producer_compression_level` (Number) The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.
- `producer_linger_ms` (Number) The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.
- `producer_queue_buffering_max_kbytes` (Number) The maximum size of the buffer in kilobytes before sending. Default: `1048576`.
- `producer_queue_buffering_max_messages` (Number) The maximum number of messages to buffer before sending. Default: `100000`.
- `producer_request_required_acks` (Number) The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.
- `skip_broken_messages` (Number) Skip at least this number of broken messages from Kafka topic per block. Default: `0`.
- `thread_per_consumer` (Boolean) Provide an independent thread for each consumer. All consumers run in the same thread by default. Default: `false`.

<a id="nestedblock--clickhouse_kafka_user_config--tables--columns"></a>
### Nested Schema for `clickhouse_kafka_user_config.tables.columns`

Required:

- `name` (String) Column name. Example: `key`.
- `type` (String) Column type. Example: `UInt64`.


<a id="nestedblock--clickhouse_kafka_user_config--tables--topics"></a>
### Nested Schema for `clickhouse_kafka_user_config.tables.topics`

Required:

- `name` (String) Name of the topic. Example: `topic_name`.




<a id="nestedblock--clickhouse_postgresql_user_config"></a>
### Nested Schema for `clickhouse_postgresql_user_config`

Optional:

- `databases` (Block List, Max: 10) Databases to expose (see [below for nested schema](#nestedblock--clickhouse_postgresql_user_config--databases))

<a id="nestedblock--clickhouse_postgresql_user_config--databases"></a>
### Nested Schema for `clickhouse_postgresql_user_config.databases`

Optional:

- `database` (String) PostgreSQL database to expose. Default: `defaultdb`.
- `schema` (String) PostgreSQL schema to expose. Default: `public`.



<a id="nestedblock--datadog_user_config"></a>
### Nested Schema for `datadog_user_config`

Optional:

- `datadog_dbm_enabled` (Boolean) Enable Datadog Database Monitoring.
- `datadog_pgbouncer_enabled` (Boolean) Enable Datadog PgBouncer Metric Tracking.
- `datadog_tags` (Block List, Max: 32) Custom tags provided by user (see [below for nested schema](#nestedblock--datadog_user_config--datadog_tags))
- `exclude_consumer_groups` (List of String) List of custom metrics.
- `exclude_topics` (List of String) List of topics to exclude.
- `include_consumer_groups` (List of String) List of custom metrics.
- `include_topics` (List of String) List of topics to include.
- `kafka_custom_metrics` (List of String) List of custom metrics.
- `max_jmx_metrics` (Number) Maximum number of JMX metrics to send. Example: `2000`.
- `mirrormaker_custom_metrics` (List of String) List of custom metrics.
- `opensearch` (Block List, Max: 1) Datadog Opensearch Options (see [below for nested schema](#nestedblock--datadog_user_config--opensearch))
- `redis` (Block List, Max: 1) Datadog Redis Options (see [below for nested schema](#nestedblock--datadog_user_config--redis))

<a id="nestedblock--datadog_user_config--datadog_tags"></a>
### Nested Schema for `datadog_user_config.datadog_tags`

Required:

- `tag` (String) Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix `aiven-` are reserved for Aiven. Example: `replica:primary`.

Optional:

- `comment` (String) Optional tag explanation. Example: `Used to tag primary replica metrics`.


<a id="nestedblock--datadog_user_config--opensearch"></a>
### Nested Schema for `datadog_user_config.opensearch`

Optional:

- `cluster_stats_enabled` (Boolean) Enable Datadog Opensearch Cluster Monitoring.
- `index_stats_enabled` (Boolean) Enable Datadog Opensearch Index Monitoring.
- `pending_task_stats_enabled` (Boolean) Enable Datadog Opensearch Pending Task Monitoring.
- `pshard_stats_enabled` (Boolean) Enable Datadog Opensearch Primary Shard Monitoring.


<a id="nestedblock--datadog_user_config--redis"></a>
### Nested Schema for `datadog_user_config.redis`

Optional:

- `command_stats_enabled` (Boolean) Enable command_stats option in the agent's configuration. Default: `false`.



<a id="nestedblock--external_aws_cloudwatch_logs_user_config"></a>
### Nested Schema for `external_aws_cloudwatch_logs_user_config`

Optional:

- `selected_log_fields` (List of String) The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.


<a id="nestedblock--external_aws_cloudwatch_metrics_user_config"></a>
### Nested Schema for `external_aws_cloudwatch_metrics_user_config`

Optional:

- `dropped_metrics` (Block List, Max: 1024) Metrics to not send to AWS CloudWatch (takes precedence over extra_metrics) (see [below for nested schema](#nestedblock--external_aws_cloudwatch_metrics_user_config--dropped_metrics))
- `extra_metrics` (Block List, Max: 1024) Metrics to allow through to AWS CloudWatch (in addition to default metrics) (see [below for nested schema](#nestedblock--external_aws_cloudwatch_metrics_user_config--extra_metrics))

<a id="nestedblock--external_aws_cloudwatch_metrics_user_config--dropped_metrics"></a>
### Nested Schema for `external_aws_cloudwatch_metrics_user_config.dropped_metrics`

Required:

- `field` (String) Identifier of a value in the metric. Example: `used`.
- `metric` (String) Identifier of the metric. Example: `java.lang:Memory`.


<a id="nestedblock--external_aws_cloudwatch_metrics_user_config--extra_metrics"></a>
### Nested Schema for `external_aws_cloudwatch_metrics_user_config.extra_metrics`

Required:

- `field` (String) Identifier of a value in the metric. Example: `used`.
- `metric` (String) Identifier of the metric. Example: `java.lang:Memory`.



<a id="nestedblock--external_elasticsearch_logs_user_config"></a>
### Nested Schema for `external_elasticsearch_logs_user_config`

Optional:

- `selected_log_fields` (List of String) The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.


<a id="nestedblock--external_opensearch_logs_user_config"></a>
### Nested Schema for `external_opensearch_logs_user_config`

Optional:

- `selected_log_fields` (List of String) The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.


<a id="nestedblock--flink_external_postgresql_user_config"></a>
### Nested Schema for `flink_external_postgresql_user_config`

Optional:

- `stringtype` (String) Enum: `unspecified`. If stringtype is set to unspecified, parameters will be sent to the server as untyped values.


<a id="nestedblock--kafka_connect_user_config"></a>
### Nested Schema for `kafka_connect_user_config`

Optional:

- `kafka_connect` (Block List, Max: 1) Kafka Connect service configuration values (see [below for nested schema](#nestedblock--kafka_connect_user_config--kafka_connect))

<a id="nestedblock--kafka_connect_user_config--kafka_connect"></a>
### Nested Schema for `kafka_connect_user_config.kafka_connect`

Optional:

- `config_storage_topic` (String) The name of the topic where connector and task configuration data are stored.This must be the same for all workers with the same group_id. Example: `__connect_configs`.
- `group_id` (String) A unique string that identifies the Connect cluster group this worker belongs to. Example: `connect`.
- `offset_storage_topic` (String) The name of the topic where connector and task configuration offsets are stored.This must be the same for all workers with the same group_id. Example: `__connect_offsets`.
- `status_storage_topic` (String) The name of the topic where connector and task configuration status updates are stored.This must be the same for all workers with the same group_id. Example: `__connect_status`.



<a id="nestedblock--kafka_logs_user_config"></a>
### Nested Schema for `kafka_logs_user_config`

Required:

- `kafka_topic` (String) Topic name. Example: `mytopic`.

Optional:

- `selected_log_fields` (List of String) The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.


<a id="nestedblock--kafka_mirrormaker_user_config"></a>
### Nested Schema for `kafka_mirrormaker_user_config`

Optional:

- `cluster_alias` (String) The alias under which the Kafka cluster is known to MirrorMaker. Can contain the following symbols: ASCII alphanumerics, `.`, `_`, and `-`. Example: `kafka-abc`.
- `kafka_mirrormaker` (Block List, Max: 1) Kafka MirrorMaker configuration values (see [below for nested schema](#nestedblock--kafka_mirrormaker_user_config--kafka_mirrormaker))

<a id="nestedblock--kafka_mirrormaker_user_config--kafka_mirrormaker"></a>
### Nested Schema for `kafka_mirrormaker_user_config.kafka_mirrormaker`

Optional:

- `consumer_auto_offset_reset` (String) Enum: `earliest`, `latest`. Set where consumer starts to consume data. Value `earliest`: Start replication from the earliest offset. Value `latest`: Start replication from the latest offset. Default is `earliest`.
- `consumer_fetch_min_bytes` (Number) The minimum amount of data the server should return for a fetch request. Example: `1024`.
- `consumer_max_poll_records` (Number) Set consumer max.poll.records. The default is 500. Example: `500`.
- `producer_batch_size` (Number) The batch size in bytes producer will attempt to collect before publishing to broker. Example: `1024`.
- `producer_buffer_memory` (Number) The amount of bytes producer can use for buffering data before publishing to broker. Example: `8388608`.
- `producer_compression_type` (String) Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. Specify the default compression type for producers. This configuration accepts the standard compression codecs (`gzip`, `snappy`, `lz4`, `zstd`). It additionally accepts `none` which is the default and equivalent to no compression.
- `producer_linger_ms` (Number) The linger time (ms) for waiting new data to arrive for publishing. Example: `100`.
- `producer_max_request_size` (Number) The maximum request size in bytes. Example: `1048576`.



<a id="nestedblock--logs_user_config"></a>
### Nested Schema for `logs_user_config`

Optional:

- `elasticsearch_index_days_max` (Number) Elasticsearch index retention limit. Default: `3`.
- `elasticsearch_index_prefix` (String) Elasticsearch index prefix. Default: `logs`.
- `selected_log_fields` (List of String) The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.


<a id="nestedblock--metrics_user_config"></a>
### Nested Schema for `metrics_user_config`

Optional:

- `database` (String) Name of the database where to store metric datapoints. Only affects PostgreSQL destinations. Defaults to `metrics`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
- `retention_days` (Number) Number of days to keep old metrics. Only affects PostgreSQL destinations. Set to 0 for no automatic cleanup. Defaults to 30 days.
- `ro_username` (String) Name of a user that can be used to read metrics. This will be used for Grafana integration (if enabled) to prevent Grafana users from making undesired changes. Only affects PostgreSQL destinations. Defaults to `metrics_reader`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
- `source_mysql` (Block List, Max: 1) Configuration options for metrics where source service is MySQL (see [below for nested schema](#nestedblock--metrics_user_config--source_mysql))
- `username` (String) Name of the user used to write metrics. Only affects PostgreSQL destinations. Defaults to `metrics_writer`. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.

<a id="nestedblock--metrics_user_config--source_mysql"></a>
### Nested Schema for `metrics_user_config.source_mysql`

Optional:

- `telegraf` (Block List, Max: 1) Configuration options for Telegraf MySQL input plugin (see [below for nested schema](#nestedblock--metrics_user_config--source_mysql--telegraf))

<a id="nestedblock--metrics_user_config--source_mysql--telegraf"></a>
### Nested Schema for `metrics_user_config.source_mysql.telegraf`

Optional:

- `gather_event_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS.
- `gather_file_events_stats` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME.
- `gather_index_io_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE.
- `gather_info_schema_auto_inc` (Boolean) Gather auto_increment columns and max values from information schema.
- `gather_innodb_metrics` (Boolean) Gather metrics from INFORMATION_SCHEMA.INNODB_METRICS.
- `gather_perf_events_statements` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST.
- `gather_process_list` (Boolean) Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
- `gather_slave_status` (Boolean) Gather metrics from SHOW SLAVE STATUS command output.
- `gather_table_io_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE.
- `gather_table_lock_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS.
- `gather_table_schema` (Boolean) Gather metrics from INFORMATION_SCHEMA.TABLES.
- `perf_events_statements_digest_text_limit` (Number) Truncates digest text from perf_events_statements into this many characters. Example: `120`.
- `perf_events_statements_limit` (Number) Limits metrics from perf_events_statements. Example: `250`.
- `perf_events_statements_time_limit` (Number) Only include perf_events_statements whose last seen is less than this many seconds. Example: `86400`.




<a id="nestedblock--prometheus_user_config"></a>
### Nested Schema for `prometheus_user_config`

Optional:

- `source_mysql` (Block List, Max: 1) Configuration options for metrics where source service is MySQL (see [below for nested schema](#nestedblock--prometheus_user_config--source_mysql))

<a id="nestedblock--prometheus_user_config--source_mysql"></a>
### Nested Schema for `prometheus_user_config.source_mysql`

Optional:

- `telegraf` (Block List, Max: 1) Configuration options for Telegraf MySQL input plugin (see [below for nested schema](#nestedblock--prometheus_user_config--source_mysql--telegraf))

<a id="nestedblock--prometheus_user_config--source_mysql--telegraf"></a>
### Nested Schema for `prometheus_user_config.source_mysql.telegraf`

Optional:

- `gather_event_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS.
- `gather_file_events_stats` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME.
- `gather_index_io_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE.
- `gather_info_schema_auto_inc` (Boolean) Gather auto_increment columns and max values from information schema.
- `gather_innodb_metrics` (Boolean) Gather metrics from INFORMATION_SCHEMA.INNODB_METRICS.
- `gather_perf_events_statements` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST.
- `gather_process_list` (Boolean) Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST.
- `gather_slave_status` (Boolean) Gather metrics from SHOW SLAVE STATUS command output.
- `gather_table_io_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE.
- `gather_table_lock_waits` (Boolean) Gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS.
- `gather_table_schema` (Boolean) Gather metrics from INFORMATION_SCHEMA.TABLES.
- `perf_events_statements_digest_text_limit` (Number) Truncates digest text from perf_events_statements into this many characters. Example: `120`.
- `perf_events_statements_limit` (Number) Limits metrics from perf_events_statements. Example: `250`.
- `perf_events_statements_time_limit` (Number) Only include perf_events_statements whose last seen is less than this many seconds. Example: `86400`.




<a id="nestedblock--timeouts"></a>
### Nested Schema for `timeouts`

Optional:

- `create` (String)
- `default` (String)
- `delete` (String)
- `read` (String)
- `update` (String)
## Import
Import is supported using the following syntax:
```shell
terraform import aiven_service_integration.example_integration PROJECT/INTEGRATION_ID
```
