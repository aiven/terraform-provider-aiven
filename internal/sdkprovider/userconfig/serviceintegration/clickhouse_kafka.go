// Code generated by user config generator. DO NOT EDIT.

package serviceintegration

import (
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"

	"github.com/aiven/terraform-provider-aiven/internal/sdkprovider/userconfig/diff"
)

func clickhouseKafkaUserConfig() *schema.Schema {
	return &schema.Schema{
		Description:      "ClickhouseKafka user configurable settings. **Warning:** There's no way to reset advanced configuration options to default. Options that you add cannot be removed later",
		DiffSuppressFunc: diff.SuppressUnchanged,
		Elem: &schema.Resource{Schema: map[string]*schema.Schema{"tables": {
			Description: "Array of table configurations that define how Kafka topics are mapped to ClickHouse tables. Each table configuration specifies the table structure, associated Kafka topics, and read/write settings",
			Elem: &schema.Resource{Schema: map[string]*schema.Schema{
				"auto_offset_reset": {
					Description:  "Enum: `beginning`, `earliest`, `end`, `largest`, `latest`, `smallest`. Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. `earliest` starts from the beginning, `latest` starts from the end. Default: `earliest`.",
					Optional:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"beginning", "earliest", "end", "largest", "latest", "smallest"}, false),
				},
				"columns": {
					Description: "Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages",
					Elem: &schema.Resource{Schema: map[string]*schema.Schema{
						"name": {
							Description: "The name of the column in the ClickHouse table. This should match the field names in your Kafka message format. Example: `key`.",
							Required:    true,
							Type:        schema.TypeString,
						},
						"type": {
							Description: "The ClickHouse data type for this column. Must be a valid ClickHouse data type that can handle the data format. Example: `UInt64`.",
							Required:    true,
							Type:        schema.TypeString,
						},
					}},
					MaxItems: 100,
					Required: true,
					Type:     schema.TypeList,
				},
				"data_format": {
					Description:  "Enum: `Avro`, `AvroConfluent`, `CSV`, `JSONAsString`, `JSONCompactEachRow`, `JSONCompactStringsEachRow`, `JSONEachRow`, `JSONStringsEachRow`, `MsgPack`, `Parquet`, `RawBLOB`, `TSKV`, `TSV`, `TabSeparated`. The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro). Default: `JSONEachRow`.",
					Required:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"Avro", "AvroConfluent", "CSV", "JSONAsString", "JSONCompactEachRow", "JSONCompactStringsEachRow", "JSONEachRow", "JSONStringsEachRow", "MsgPack", "Parquet", "RawBLOB", "TSKV", "TSV", "TabSeparated"}, false),
				},
				"date_time_input_format": {
					Description:  "Enum: `basic`, `best_effort`, `best_effort_us`. Specifies how ClickHouse should parse DateTime values from text-based input formats. `basic` uses simple parsing, `best_effort` attempts more flexible parsing. Default: `basic`.",
					Optional:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"basic", "best_effort", "best_effort_us"}, false),
				},
				"group_name": {
					Description: "The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions. Default: `clickhouse`.",
					Required:    true,
					Type:        schema.TypeString,
				},
				"handle_error_mode": {
					Description:  "Enum: `default`, `stream`. Defines how ClickHouse should handle errors when processing Kafka messages. `default` stops on errors, `stream` continues processing and logs errors. Default: `default`.",
					Optional:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"default", "stream"}, false),
				},
				"max_block_size": {
					Description: "Maximum number of rows to collect before flushing data between Kafka and ClickHouse. Default: `0`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"max_rows_per_message": {
					Description: "Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage. Default: `1`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"name": {
					Description: "The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics. Example: `events`.",
					Required:    true,
					Type:        schema.TypeString,
				},
				"num_consumers": {
					Description: "Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage. Default: `1`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"poll_max_batch_size": {
					Description: "Maximum number of messages to fetch in a single Kafka poll operation for reading. Default: `0`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"poll_max_timeout_ms": {
					Description: "Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream_flush_interval_ms server setting by default (500ms). Default: `0`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_batch_num_messages": {
					Description: "The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent. Default: `10000`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_batch_size": {
					Description: "The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent. Default: `1000000`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_compression_codec": {
					Description:  "Enum: `gzip`, `lz4`, `none`, `snappy`, `zstd`. The compression codec to use when sending a batch of messages to Kafka. Default: `none`.",
					Optional:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"gzip", "lz4", "none", "snappy", "zstd"}, false),
				},
				"producer_compression_level": {
					Description: "The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Default: `-1`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_linger_ms": {
					Description: "The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent. Default: `5`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_queue_buffering_max_kbytes": {
					Description: "The maximum size of the buffer in kilobytes before sending. Default: `1048576`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_queue_buffering_max_messages": {
					Description: "The maximum number of messages to buffer before sending. Default: `100000`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"producer_request_required_acks": {
					Description: "The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs). Default: `-1`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"skip_broken_messages": {
					Description: "Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration. Default: `0`.",
					Optional:    true,
					Type:        schema.TypeInt,
				},
				"thread_per_consumer": {
					Description: "When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios. Default: `false`.",
					Optional:    true,
					Type:        schema.TypeBool,
				},
				"topics": {
					Description: "Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics",
					Elem: &schema.Resource{Schema: map[string]*schema.Schema{"name": {
						Description: "The name of the Kafka topic to read messages from or write messages to. The topic must exist in the Kafka cluster. Example: `topic_name`.",
						Required:    true,
						Type:        schema.TypeString,
					}}},
					MaxItems: 100,
					Required: true,
					Type:     schema.TypeList,
				},
			}},
			MaxItems: 400,
			Optional: true,
			Type:     schema.TypeList,
		}}},
		MaxItems: 1,
		Optional: true,
		Type:     schema.TypeList,
	}
}
